{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True  # Set to True to enable debugging, False to disable\n",
    "\n",
    "def debug_message(msg: str):\n",
    "    \"\"\"Prints a debug message only if DEBUG is True.\"\"\"\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "# Initialize API keys\n",
    "import os\n",
    "\n",
    "# Hugging Face API key\n",
    "os.environ['HUGGINGFACE_API_KEY'] = 'your_huggingface_api_key_here'\n",
    "\n",
    "# LangChain API key\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'your_langchain_api_key_here'\n",
    "\n",
    "# OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'\n",
    "\n",
    "# Groq API key\n",
    "os.environ['GROQ_API_KEY'] = 'your_groq_api_key_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = r\"/path/to/your/dataset_directory\"\n",
    "loader = DirectoryLoader(dataset_path, glob=\"*.txt\", loader_cls=TextLoader, show_progress=True, use_multithreading=True)\n",
    "\n",
    "# Get the list of files and sort them\n",
    "file_list = sorted(os.listdir(dataset_path), key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "# Load files\n",
    "file_list = file_list[:10]\n",
    "\n",
    "# Load documents\n",
    "docs = [doc for file in file_list for doc in TextLoader(os.path.join(dataset_path, file)).load()]\n",
    "\n",
    "# Display the first few documents for verification\n",
    "for doc in docs[:1]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Determine the dimensionality of the embeddings dynamically\n",
    "embedding_dim = len(embeddings.embed_query(\"\"))\n",
    "\n",
    "# Initialize FAISS index\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# Prepare documents with the correct structure\n",
    "documents = [\n",
    "    Document(page_content=doc.page_content, metadata=doc.metadata)\n",
    "    for doc in docs  # Convert your `docs` to the expected `Document` format\n",
    "]\n",
    "\n",
    "# Generate unique identifiers for each document\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "# Add documents to the vector store\n",
    "vectorstore.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "print(f\"Successfully added {len(documents)} documents to the FAISS vector store.\")\n",
    "\n",
    "# Save and load the FAISS vector store\n",
    "if not os.path.exists(\"faiss_index\"):\n",
    "    vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "if os.path.exists(\"faiss_index\"):\n",
    "    new_vector_store = FAISS.load_local(\n",
    "        \"faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to chunk the document\n",
    "def chunk_document(doc_content):\n",
    "    sections = doc_content.split(\"\\n\\n\")\n",
    "    chunks = [section for section in sections]\n",
    "    debug_message(f\"Document chunked into {len(chunks)} sections.\")\n",
    "    return chunks\n",
    "\n",
    "# Function to retrieve specific report based on the question\n",
    "def retrieve_specific_report(question):\n",
    "    # Match either M, L, Q followed by digits, or just digits\n",
    "    match = re.search(r'\\b(?:[MLQ]?)(\\d+)\\b', question)  # Updated regex to make M, L, Q optional\n",
    "    if match:\n",
    "        report_num = match.group(1)  # Capture only the numeric part\n",
    "        debug_message(f\"Extracted report number: {report_num}\")\n",
    "\n",
    "        # Construct the report path\n",
    "        report_path = os.path.join(\n",
    "            r\"/path/to/your/dataset_directory\",\n",
    "            f\"report_{report_num}.txt\"\n",
    "        )\n",
    "        debug_message(f\"Constructed report path: {report_path}\")\n",
    "\n",
    "        if os.path.exists(report_path):\n",
    "            debug_message(\"File exists.\")\n",
    "            loader = TextLoader(report_path)\n",
    "            doc = loader.load()[0]\n",
    "            debug_message(f\"Loaded document: {doc.page_content[:100]}...\")  # Preview of loaded content\n",
    "            return doc\n",
    "        else:\n",
    "            debug_message(\"File does not exist.\")\n",
    "    else:\n",
    "        debug_message(\"No match found for report identifier.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the diagnosis of patient 0?\"\n",
    "\n",
    "# Retrieve specific report based on the question\n",
    "specific_doc = retrieve_specific_report(question)\n",
    "\n",
    "if specific_doc:\n",
    "    # Chunk the document\n",
    "    chunks = chunk_document(specific_doc.page_content)\n",
    "    print(chunks)\n",
    "else:\n",
    "    print(\"No specific report found for the given question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Initialize the cross-encoder model\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# Rerank the chunks based on the question\n",
    "reranked_scores = cross_encoder.predict([(question, chunk) for chunk in chunks])\n",
    "\n",
    "# Add scores to the chunks\n",
    "chunk_scores = list(zip(chunks, reranked_scores))\n",
    "\n",
    "# Sort chunks by score\n",
    "sorted_chunks = sorted(chunk_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the scores for debugging purposes\n",
    "for chunk, score in sorted_chunks:\n",
    "    print(f\"Score: {score}\\nChunk: {chunk}\\n\")\n",
    "\n",
    "# Get the top chunk\n",
    "top_chunk = sorted_chunks[0][0]\n",
    "\n",
    "print(top_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one\n",
    "# 1\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "# llm = ChatGroq(\n",
    "#     model=\"gemma2-9b-it\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=2,\n",
    "# )\n",
    "\n",
    "# llm_rewrite = ChatGroq(\n",
    "#     model=\"gemma2-9b-it\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=2,\n",
    "# )\n",
    "\n",
    "# 2\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key='your_openai_api_key_here'\n",
    ")\n",
    "\n",
    "llm_rewrite = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key='your_openai_api_key_here'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a highly efficient and detail-oriented medical assistant. \"\n",
    "        \"Provide direct, concise, and accurate answers based on the provided information, specifically addressing the mentioned patient Medical Record Number (MRN). Ensure responses include all relevant details, with a clear mention of the stage of the disease when applicable. Avoid unnecessary explanations, apologies, or corrections, focusing solely on delivering precise and complete information.\"\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to store the context of the conversation\n",
    "context = {\n",
    "    \"mrns\": [],\n",
    "    \"docs\": {},\n",
    "    \"specific_docs\": {},\n",
    "    \"chunks\": {},\n",
    "    \"top_chunks\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "class Util:\n",
    "    @staticmethod\n",
    "    def extract_mrns(question):\n",
    "        \"\"\"Extract patient MRNs (with or without prefixes) from a question.\"\"\"\n",
    "        mrns = re.findall(r'\\b(?:[MLQ]?)(\\d+)\\b', question)\n",
    "        debug_message(f\"Extracted MRNs: {mrns}\")\n",
    "        return mrns\n",
    "\n",
    "    @staticmethod\n",
    "    def resolve_mrns(question):\n",
    "        \"\"\"\n",
    "        Extract patient MRNs from the question. If none are found, fallback to\n",
    "        the MRNs stored in 'context[\"mrns\"]'.\n",
    "        \"\"\"\n",
    "        new_mrns = Util.extract_mrns(question)\n",
    "        \n",
    "        if new_mrns:\n",
    "            context[\"mrns\"] = new_mrns\n",
    "            debug_message(f\"New MRNs found and updated in context: {new_mrns}\")\n",
    "        else:\n",
    "            if context[\"mrns\"]:\n",
    "                debug_message(f\"No new MRNs found; reusing existing: {context['mrns']}\")\n",
    "            else:\n",
    "                debug_message(\"No MRNs found in question, and none were in context.\")\n",
    "        \n",
    "        return context[\"mrns\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Retrieval\n",
    "class DocRetriever:\n",
    "    \"\"\"\n",
    "    Handles retrieval of documents for specific patient MRNs.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve_doc(patient_mrn):\n",
    "        \"\"\"\n",
    "        Retrieve a document for a specific patient MRN.\n",
    "\n",
    "        Args:\n",
    "            patient_mrn (str): The medical record number of the patient.\n",
    "\n",
    "        Returns:\n",
    "            Document: The retrieved document or None if not found.\n",
    "        \"\"\"\n",
    "        debug_message(f\"[DocRetriever] Action: Retrieving document | Patient MRN: {patient_mrn}\")\n",
    "        doc = retrieve_specific_report(patient_mrn)\n",
    "        if doc:\n",
    "            context.setdefault(\"docs\", {})[patient_mrn] = doc\n",
    "            debug_message(f\"[DocRetriever] Status: Document retrieved | Patient MRN: {patient_mrn} | Document Preview: {doc.page_content[:100]}...\")\n",
    "            return doc\n",
    "        debug_message(f\"[DocRetriever] Status: No report found | Patient MRN: {patient_mrn}\")\n",
    "        return None\n",
    "\n",
    "# Document Chunking\n",
    "class DocChunker:\n",
    "    \"\"\"\n",
    "    Handles chunking and storing of document data for efficient processing.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_store_doc(patient_mrn, doc):\n",
    "        \"\"\"\n",
    "        Chunk a specific document and store the chunks in the context.\n",
    "\n",
    "        Args:\n",
    "            patient_mrn (str): The medical record number of the patient.\n",
    "            doc (Document): The document to chunk.\n",
    "\n",
    "        Returns:\n",
    "            list: The list of document chunks.\n",
    "        \"\"\"\n",
    "        chunks = chunk_document(doc.page_content)\n",
    "        context.setdefault(\"chunks\", {})[patient_mrn] = chunks\n",
    "        debug_message(f\"[DocChunker] Action: Storing chunks | Patient MRN: {patient_mrn} | Total Chunks: {len(chunks)}\")\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve_chunk_doc(patient_mrn):\n",
    "        \"\"\"\n",
    "        Retrieve a document and chunk it if not already stored in context.\n",
    "\n",
    "        Args:\n",
    "            patient_mrn (str): The medical record number of the patient.\n",
    "\n",
    "        Returns:\n",
    "            list: The list of chunks for the document or an empty list if no document is found.\n",
    "        \"\"\"\n",
    "        if patient_mrn not in context.get(\"docs\", {}):\n",
    "            doc = DocRetriever.retrieve_doc(patient_mrn)\n",
    "            if not doc:\n",
    "                debug_message(f\"[DocChunker] Status: No document found for patient MRN: {patient_mrn}\")\n",
    "                return None\n",
    "            return DocChunker.chunk_store_doc(patient_mrn, doc)\n",
    "        debug_message(f\"[DocChunker] Status: Chunks retrieved from context for patient MRN: {patient_mrn}\")\n",
    "        return context.get(\"chunks\", {}).get(patient_mrn, [])\n",
    "\n",
    "# Chunk Reranking\n",
    "class ChunkReranker:\n",
    "    \"\"\"\n",
    "    Handles reranking of document chunks based on their relevance to a given question.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def rerank(question, chunks):\n",
    "        \"\"\"\n",
    "        Rerank document chunks based on their relevance to the provided question.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to evaluate relevance.\n",
    "            chunks (list of str): The document chunks to rank.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tuples with chunks and their relevance scores, sorted in descending order.\n",
    "        \"\"\"\n",
    "        debug_message(f\"[ChunkReranker] Action: Reranking chunks | Question: {question}\")\n",
    "        scores = cross_encoder.predict([(question, chunk) for chunk in chunks])\n",
    "\n",
    "        # Pair chunks with their scores\n",
    "        chunk_scores = list(zip(chunks, scores))\n",
    "        debug_message(f\"[ChunkReranker] Chunk Scores:\")\n",
    "        for chunk, score in chunk_scores:\n",
    "            debug_message(f\"Chunk: {chunk[:300]}... | Score: {score:.2f}\")\n",
    "\n",
    "        # Sort chunks by score in descending order\n",
    "        sorted_chunks = sorted(chunk_scores, key=lambda x: x[1], reverse=True)\n",
    "        debug_message(f\"[ChunkReranker] Sorted Chunks:\")\n",
    "        for chunk, score in sorted_chunks:\n",
    "            debug_message(f\"Chunk: {chunk[:300]}... | Score: {score:.2f}\")\n",
    "\n",
    "        return sorted_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "class SingleQA:\n",
    "    \"\"\"\n",
    "    Handles answering a single question for a specific patient by leveraging retrieved and ranked document chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def answer(patient_mrn, question):\n",
    "        \"\"\"\n",
    "        Retrieve and generate an answer for a given patient and question.\n",
    "\n",
    "        Args:\n",
    "            patient_mrn (str): The medical record number of the patient.\n",
    "            question (str): The question to answer.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated answer or an error message if no data is found.\n",
    "        \"\"\"\n",
    "        debug_message(f\"[SingleQA] Action: Retrieving answer | Patient MRN: {patient_mrn}\")\n",
    "\n",
    "        # Retrieve chunks for the patient\n",
    "        if patient_mrn not in context.get(\"chunks\", {}):\n",
    "            debug_message(f\"[SingleQA] Status: Chunks not found in memory | Patient MRN: {patient_mrn}. Retrieving and chunking document.\")\n",
    "            chunks = DocChunker.retrieve_chunk_doc(patient_mrn)\n",
    "            if not chunks:\n",
    "                error_message = f\"No report or chunks found for patient MRN: {patient_mrn}.\"\n",
    "                debug_message(f\"[SingleQA] Status: {error_message}\")\n",
    "                return error_message\n",
    "            context.setdefault(\"chunks\", {})[patient_mrn] = chunks\n",
    "\n",
    "        chunks = context[\"chunks\"][patient_mrn]\n",
    "\n",
    "        # Rerank chunks based on the question\n",
    "        debug_message(f\"[SingleQA] Action: Reranking chunks | Patient MRN: {patient_mrn}\")\n",
    "        sorted_chunks = ChunkReranker.rerank(question, chunks)\n",
    "        top_chunk = sorted_chunks[0][0]\n",
    "        context.setdefault(\"top_chunks\", {})[patient_mrn] = top_chunk\n",
    "\n",
    "        # Generate response using the top chunk\n",
    "        input_msgs = [HumanMessage(question), HumanMessage(top_chunk)]\n",
    "        debug_message(f\"[SingleQA] Action: Invoking app for response generation | Patient MRN: {patient_mrn}\")\n",
    "        output = app.invoke({\"messages\": input_msgs}, config)\n",
    "        answer = output[\"messages\"][-1].content\n",
    "\n",
    "        debug_message(f\"[SingleQA] Status: Answer generated | Patient MRN: {patient_mrn} | Answer: {answer}\")\n",
    "        return answer\n",
    "\n",
    "\n",
    "class MultiQA:\n",
    "    \"\"\"\n",
    "    Handles multi-patient questions by dividing questions, generating individual answers,\n",
    "    and summarizing the results.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def divide(question, patient_mrns):\n",
    "        \"\"\"\n",
    "        Rewrite the question for each patient.\n",
    "\n",
    "        Args:\n",
    "            question (str): The original question.\n",
    "            patient_mrns (list of str): List of patient MRNs to customize the question for.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary mapping each patient MRN to their customized question.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no questions could be generated for the patients.\n",
    "        \"\"\"\n",
    "        divide_prompt = (\n",
    "            f\"Rewrite the question: '{question}' for each of these patients: \"\n",
    "            f\"{', '.join(patient_mrns)}. \"\n",
    "            \"Include each patient's MRN in the rewritten question.\"\n",
    "        )\n",
    "\n",
    "        debug_message(\"[MultiQA] Action: Generating patient-specific questions.\")\n",
    "        input_msgs = [HumanMessage(divide_prompt)]\n",
    "        output = llm_rewrite.invoke(input_msgs, config)\n",
    "\n",
    "        divided_questions = output.content.split('\\n')\n",
    "        questions_dict = {}\n",
    "        for line in divided_questions:\n",
    "            for patient_mrn in patient_mrns:\n",
    "                if patient_mrn in line:\n",
    "                    questions_dict[patient_mrn] = line.strip()\n",
    "                    break\n",
    "\n",
    "        if not questions_dict:\n",
    "            error_message = \"Failed to divide the question into patient-specific queries.\"\n",
    "            debug_message(f\"[MultiQA] Status: {error_message}\")\n",
    "            raise ValueError(error_message)\n",
    "\n",
    "        debug_message(f\"[MultiQA] Status: Divided questions generated | Questions: {questions_dict}\")\n",
    "        return questions_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def summarize(all_answers):\n",
    "        \"\"\"\n",
    "        Summarize answers for multiple patients.\n",
    "\n",
    "        Args:\n",
    "            all_answers (list of str): List of answers for individual patients.\n",
    "\n",
    "        Returns:\n",
    "            str: A summarized response for all answers.\n",
    "        \"\"\"\n",
    "        combined_answers = \"\\n\\n\".join(all_answers)\n",
    "        debug_message(f\"[MultiQA] Action: Summarizing answers | Combined Answers: {combined_answers}\")\n",
    "\n",
    "        summary_question = \"Provide a direct summary of the answers without any introductory text.\"\n",
    "        input_msgs = [HumanMessage(summary_question), HumanMessage(combined_answers)]\n",
    "\n",
    "        summary_output = app.invoke({\"messages\": input_msgs}, config)\n",
    "        summary = summary_output[\"messages\"][-1].content\n",
    "\n",
    "        debug_message(f\"[MultiQA] Status: Summary generated | Summary: {summary}\")\n",
    "        return summary\n",
    "\n",
    "    @staticmethod\n",
    "    def multi_answer(question, patient_mrns):\n",
    "        \"\"\"\n",
    "        Generate answers for multiple patients and summarize the results.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to answer.\n",
    "            patient_mrns (list of str): List of patient MRNs to answer for.\n",
    "\n",
    "        Returns:\n",
    "            str: A summarized answer for all patients.\n",
    "        \"\"\"\n",
    "        debug_message(\"[MultiQA] Action: Initiating multi-answer process for multiple patients.\")\n",
    "\n",
    "        # Divide the question into patient-specific queries\n",
    "        divided_questions = MultiQA.divide(question, patient_mrns)\n",
    "        all_answers = []\n",
    "\n",
    "        for patient_mrn, patient_question in divided_questions.items():\n",
    "            debug_message(f\"[MultiQA] Action: Processing question for patient MRN {patient_mrn} | Question: {patient_question}\")\n",
    "            answer = SingleQA.answer(patient_mrn, patient_question)\n",
    "            all_answers.append(f\"Patient {patient_mrn}: {answer}\")\n",
    "\n",
    "        debug_message(\"[MultiQA] Action: Combining and summarizing all answers.\")\n",
    "        return MultiQA.summarize(all_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    \"\"\"\n",
    "    A class for generating summaries of medical reports for patients.\n",
    "    Includes functionality to summarize entire reports, specific sections, \n",
    "    and multiple patients' reports.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def summarize_report(patient_mrn):\n",
    "        \"\"\"\n",
    "        Summarize the entire report for a single patient report.\n",
    "\n",
    "        Args:\n",
    "            patient_mrn (str): The medical record number of the patient.\n",
    "\n",
    "        Returns:\n",
    "            str: A structured summary of the patient's report or an error message if no report is found.\n",
    "        \"\"\"\n",
    "        debug_message(f\"[Summarizer] Action: Starting summarization for patient MRN: {patient_mrn}\")\n",
    "\n",
    "        # Retrieve the document for the patient\n",
    "        doc = DocRetriever.retrieve_doc(patient_mrn)\n",
    "        if not doc:\n",
    "            error_message = f\"No report found for patient MRN: {patient_mrn}.\"\n",
    "            debug_message(f\"[Summarizer] Status: {error_message}\")\n",
    "            return error_message\n",
    "\n",
    "        debug_message(f\"[Summarizer] Status: Document retrieved | Patient MRN: {patient_mrn}\")\n",
    "        debug_message(doc.page_content[:500] + \"...\" if len(doc.page_content) > 500 else doc.page_content)\n",
    "\n",
    "        # Build the summarization prompt\n",
    "        input_msgs = [HumanMessage(\n",
    "            f\"You are a highly skilled medical assistant specializing in summarizing medical reports. \"\n",
    "            \"Your task is to create concise, structured, and professional summaries intended for busy medical professionals. \"\n",
    "            \"Follow this specific format, but only include sections present in the original report. \"\n",
    "            \"Do not add subsections not explicitly mentioned in the report. \"\n",
    "            \"\\n\\n\"\n",
    "            \"### Patient Summary Report\\n\\n\"\n",
    "            \"#### Patient Information (if provided in the report)\\n\"\n",
    "            \"- **Name:** [Patient's name]\\n\"\n",
    "            \"- **Date of Birth:** [Patient's DOB]\\n\"\n",
    "            \"- **Admission Dates:** [Admission and discharge dates]\\n\"\n",
    "            \"- **Primary Diagnosis:** [Diagnosis]\\n\"\n",
    "            \"- **Attending Physician:** [Physician's name]\\n\\n\"\n",
    "            \"#### Reason for Admission (if provided in the report)\\n\"\n",
    "            \"Provide a brief description of the primary symptoms and reasons for admission, \"\n",
    "            \"including any relevant context about delays in seeking care if applicable.\\n\\n\"\n",
    "            \"#### Medical History (if provided in the report)\\n\"\n",
    "            \"- Briefly list relevant medical, family, and social history (e.g., smoking, hypertension, family illnesses).\\n\\n\"\n",
    "            \"#### Diagnostic Findings (if provided in the report)\\n\"\n",
    "            \"Summarize key diagnostic tests and results, grouped by type (e.g., biopsy, imaging, blood tests). \"\n",
    "            \"Use bullet points for clarity.\\n\\n\"\n",
    "            \"#### Treatment Plan (if provided in the report)\\n\"\n",
    "            \"Outline the treatments administered during the hospital stay, \"\n",
    "            \"including chemotherapy, surgery, radiation, and palliative care. Use concise bullet points.\\n\\n\"\n",
    "            \"#### Hospital Course (if provided in the report)\\n\"\n",
    "            \"Summarize significant events and complications during the hospital stay, \"\n",
    "            \"including patient response to treatments.\\n\\n\"\n",
    "            \"#### Follow-Up Plan (if provided in the report)\\n\"\n",
    "            \"Detail the post-discharge follow-up plan, such as chemotherapy schedules, imaging, blood tests, and supportive care.\\n\\n\"\n",
    "            \"#### Discharge Instructions (if provided in the report)\\n\"\n",
    "            \"Provide any specific discharge instructions given to the patient, including medication, diet, and activity recommendations.\\n\\n\"\n",
    "            \"#### Prognosis and Long-Term Outlook (if provided in the report)\\n\"\n",
    "            \"Provide a brief prognosis and any long-term outlook information, including survival rates if applicable.\\n\\n\"\n",
    "            \"#### Final Remarks (if provided in the report)\\n\"\n",
    "            \"Include any final remarks or recommendations provided by the attending physician.\\n\\n\"\n",
    "            f\"{doc.page_content}\"\n",
    "        )]\n",
    "\n",
    "        # Generate summary\n",
    "        debug_message(\"[Summarizer] Action: Invoking app for summarization.\")\n",
    "        summary_output = app.invoke({\"messages\": input_msgs}, config)\n",
    "        summary = summary_output[\"messages\"][-1].content\n",
    "\n",
    "        debug_message(f\"[Summarizer] Status: Summary generated | Patient MRN: {patient_mrn}\")\n",
    "        debug_message(summary[:500] + \"...\" if len(summary) > 500 else summary)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    @staticmethod\n",
    "    def summarize_section(patient_mrn, section_title):\n",
    "        \"\"\"\n",
    "        Summarize a specific section of a patient's report.\n",
    "\n",
    "        Args:\n",
    "            patient_mrn (str): The medical record number of the patient.\n",
    "            section_title (str): The title of the section to summarize.\n",
    "\n",
    "        Returns:\n",
    "            str: A summary of the specified section or an error message if the section or report is not found.\n",
    "        \"\"\"\n",
    "        debug_message(f\"[Summarizer] Action: Starting section summarization | Patient MRN: {patient_mrn} | Section: '{section_title}'\")\n",
    "\n",
    "        # Retrieve the document for the patient\n",
    "        doc = DocRetriever.retrieve_doc(patient_mrn)\n",
    "        if not doc:\n",
    "            error_message = f\"No report found for patient MRN: {patient_mrn}.\"\n",
    "            debug_message(f\"[Summarizer] Status: {error_message}\")\n",
    "            return error_message\n",
    "\n",
    "        # Build the section-specific prompt\n",
    "        input_msgs = [HumanMessage(f\"Summarize the section titled '{section_title}' in the report for patient {patient_mrn}.\")]\n",
    "\n",
    "        # Generate section summary\n",
    "        debug_message(\"[Summarizer] Action: Invoking app for section summarization.\")\n",
    "        output = app.invoke({\"messages\": input_msgs}, config)\n",
    "        summary = output[\"messages\"][-1].content\n",
    "\n",
    "        debug_message(f\"[Summarizer] Status: Section summary generated | Patient MRN: {patient_mrn} | Section: '{section_title}'\")\n",
    "        debug_message(summary[:500] + \"...\" if len(summary) > 500 else summary)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    @staticmethod\n",
    "    def summarize_reports(patient_mrns):\n",
    "        \"\"\"\n",
    "        Generate summaries for multiple patient reports.\n",
    "\n",
    "        Args:\n",
    "            patient_mrns (list of str): List of medical record numbers for the patients.\n",
    "\n",
    "        Returns:\n",
    "            str: Summarized reports for all patients combined into a single string.\n",
    "        \"\"\"\n",
    "        debug_message(\"[Summarizer] Action: Starting summarization for multiple patients.\")\n",
    "\n",
    "        all_summaries = []\n",
    "        for patient_mrn in patient_mrns:\n",
    "            debug_message(f\"[Summarizer] Action: Processing summarization | Patient MRN: {patient_mrn}\")\n",
    "            summary = Summarizer.summarize_report(patient_mrn)\n",
    "            all_summaries.append(f\"Patient {patient_mrn}:\\n{summary}\")\n",
    "\n",
    "        combined_summaries = \"\\n\\n\".join(all_summaries)\n",
    "        debug_message(\"[Summarizer] Status: Completed summarization for all patients.\")\n",
    "        debug_message(combined_summaries[:500] + \"...\" if len(combined_summaries) > 500 else combined_summaries)\n",
    "\n",
    "        return combined_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarFinder:\n",
    "    \"\"\"\n",
    "    A class to find and summarize cases similar to a specified patient \n",
    "    by leveraging document embeddings and a FAISS vector store.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def find_similar(patient_mrn, k=5):\n",
    "        \"\"\"\n",
    "        Find patients with similar cases to the specified patient using doc-level embeddings.\n",
    "\n",
    "        Args:\n",
    "            patient_mrn (str): The medical record number of the patient to find similar cases for.\n",
    "            k (int): The number of similar cases to retrieve (default is 5).\n",
    "\n",
    "        Returns:\n",
    "            str: A summary of similar cases, highlighting similarities and differences, or an error message.\n",
    "        \"\"\"\n",
    "        debug_message(f\"[SimilarFinder] Action: Finding patients similar to patient {patient_mrn}\")\n",
    "\n",
    "        # Retrieve the document for the given patient MRN\n",
    "        doc = DocRetriever.retrieve_doc(patient_mrn)\n",
    "        if not doc:\n",
    "            error_message = f\"No report found for patient {patient_mrn}.\"\n",
    "            debug_message(f\"[SimilarFinder] Status: {error_message}\")\n",
    "            return error_message\n",
    "\n",
    "        # Generate an embedding for the patient's report\n",
    "        debug_message(\"[SimilarFinder] Generating embedding for patient report.\")\n",
    "        patient_embed = embeddings.embed_query(doc.page_content)\n",
    "\n",
    "        # Search the doc-level vector store for top-k similar documents\n",
    "        debug_message(f\"[SimilarFinder] Searching for top {k} similar documents in vector store.\")\n",
    "        distances, indices = vectorstore.index.search(\n",
    "            np.array([patient_embed]).astype('float32'), k\n",
    "        )\n",
    "\n",
    "        # Retrieve the documents for the similar cases\n",
    "        similar_docs = []\n",
    "        debug_message(\"[SimilarFinder] Retrieved the following similar documents:\")\n",
    "        for idx in indices[0]:\n",
    "            doc_id = vectorstore.index_to_docstore_id.get(idx)\n",
    "            if doc_id:\n",
    "                document = vectorstore.docstore.search(doc_id)\n",
    "                if document:\n",
    "                    similar_docs.append(document)\n",
    "                    debug_message(f\"Doc ID: {doc_id} | Content Preview: {document.page_content[:300]}...\")\n",
    "\n",
    "        if not similar_docs:\n",
    "            error_message = \"No similar documents found in the vector store.\"\n",
    "            debug_message(f\"[SimilarFinder] Status: {error_message}\")\n",
    "            return error_message\n",
    "\n",
    "        debug_message(f\"[SimilarFinder] Found {len(similar_docs)} similar documents.\")\n",
    "\n",
    "        # Optional: Summarize each retrieved document\n",
    "        summaries = []\n",
    "        for similar_doc in similar_docs:\n",
    "            doc_text = similar_doc.page_content\n",
    "            doc_id = similar_doc.metadata.get('doc_id', 'Unknown ID')\n",
    "            debug_message(f\"[SimilarFinder] Summarizing report for document ID: {doc_id}\")\n",
    "\n",
    "            # Truncate or chunk document text to a manageable size\n",
    "            truncated_text = doc_text[:1500]\n",
    "            input_msgs = [\n",
    "                HumanMessage(content=f\"Summarize this medical report:\\n{truncated_text}\")\n",
    "            ]\n",
    "            summary_response = app.invoke({\"messages\": input_msgs}, config)\n",
    "            summaries.append(summary_response[\"messages\"][-1].content)\n",
    "\n",
    "        # Combine summaries and generate a final comparison\n",
    "        combined_summaries = \"\\n\\n\".join(summaries)\n",
    "        final_summary_prompt = (\n",
    "            \"Combine and summarize the following patient cases, focusing on their similarities and differences:\\n\\n\"\n",
    "            f\"{combined_summaries}\"\n",
    "        )\n",
    "        debug_message(\"[SimilarFinder] Generating final summary for similar cases.\")\n",
    "        final_summary_response = app.invoke({\"messages\": [HumanMessage(content=final_summary_prompt)]}, config)\n",
    "\n",
    "        final_summary = final_summary_response[\"messages\"][-1].content\n",
    "        debug_message(f\"[SimilarFinder] Final summary generated:\")\n",
    "        debug_message(final_summary[:500] + \"...\" if len(final_summary) > 500 else final_summary)\n",
    "\n",
    "        return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "reference_examples = {\n",
    "    \"single_qa\": [\n",
    "        \"What is the diagnosis for patient 0?\",\n",
    "        \"What is the date of admission for patient 1?\",\n",
    "        \"Tell me the treatment plan for patient 2.\",\n",
    "        \"What are the findings for patient 3?\",\n",
    "        \"What medications is patient 4 taking?\",\n",
    "        \"What is the discharge summary for patient 0?\",\n",
    "        \"What is the medical history of patient 1?\",\n",
    "        \"What are the lab results for patient 2?\",\n",
    "        \"What is the prognosis for patient 3?\",\n",
    "        \"What surgeries has patient 4 undergone?\"\n",
    "    ],\n",
    "    \"multi_qa\": [\n",
    "        \"Compare the reports for patients 0 and 1.\",\n",
    "        \"What are the diagnoses for patients 2 and 3?\",\n",
    "        \"What is the treatment plan for both patients 0 and 1?\",\n",
    "        \"When did patients 2 and 3 get discharged?\",\n",
    "        \"What are the names of patients 0 and 1?\",\n",
    "        \"What are the symptoms for patients 0 and 1?\",\n",
    "        \"Compare the lab results of patients 2 and 3.\",\n",
    "        \"What are the medications for patients 0 and 1?\",\n",
    "        \"What are the findings for patients 2 and 3?\",\n",
    "        \"What is the prognosis for patients 0 and 1?\"\n",
    "    ],\n",
    "    \"similar_patient\": [\n",
    "        \"Find patients with cases similar to patient 0.\",\n",
    "        \"Who has a similar diagnosis as patient 1?\",\n",
    "        \"Show patients with the same condition as patient 2.\",\n",
    "        \"Find patients with cases similar to patient 3.\",\n",
    "        \"Who has a similar treatment plan as patient 4?\",\n",
    "        \"Find patients with similar symptoms to patient 0.\",\n",
    "        \"Who has a similar medical history as patient 1?\",\n",
    "        \"Show patients with similar lab results as patient 2.\",\n",
    "        \"Find patients with similar prognoses to patient 3.\",\n",
    "        \"Who has undergone similar surgeries as patient 4?\"\n",
    "    ],\n",
    "    \"summarization\": [\n",
    "        \"Summarize the report for patient 0.\",\n",
    "        \"What are the key points from patient 1's file?\",\n",
    "        \"Give me an overview of the report for patient 2.\",\n",
    "        \"Summarize the report for patient 3.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Question classifier and LLM response\n",
    "class IntentClassifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the IntentClassifier with a pretrained SentenceTransformer model \n",
    "        and reference embeddings for intent classification.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "        self.ref_embeds = self._embed_refs()\n",
    "\n",
    "    def _embed_refs(self):\n",
    "        \"\"\"\n",
    "        Embed all reference examples and store them for similarity comparisons.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary mapping intents to their embeddings.\n",
    "        \"\"\"\n",
    "        debug_message(\"[IntentClassifier] Action: Embedding reference examples.\")\n",
    "        embeds = {}\n",
    "        for intent, examples in reference_examples.items():\n",
    "            embeds[intent] = self.model.encode(examples, convert_to_tensor=True)\n",
    "        debug_message(\"[IntentClassifier] Status: Reference examples embedded successfully.\")\n",
    "        return embeds\n",
    "\n",
    "    def classify(self, question):\n",
    "        \"\"\"\n",
    "        Classify the intent of a question using similarity to reference examples.\n",
    "\n",
    "        Args:\n",
    "            question (str): The input question to classify.\n",
    "\n",
    "        Returns:\n",
    "            str: The intent with the highest similarity to the question.\n",
    "        \"\"\"\n",
    "        debug_message(f\"[IntentClassifier] Action: Classifying question intent | Question: {question}\")\n",
    "        question_embed = self.model.encode(question, convert_to_tensor=True)\n",
    "\n",
    "        # Compare question embedding to reference embeddings\n",
    "        intent_scores = {}\n",
    "        for intent, embeds in self.ref_embeds.items():\n",
    "            similarities = util.pytorch_cos_sim(question_embed, embeds)\n",
    "            avg_similarity = similarities.mean().item()\n",
    "            intent_scores[intent] = avg_similarity\n",
    "\n",
    "        # Find the intent with the highest average similarity\n",
    "        best_intent = max(intent_scores, key=intent_scores.get)\n",
    "        debug_message(f\"[IntentClassifier] Status: Classified intent | Intent: {best_intent} | Scores: {intent_scores}\")\n",
    "        return best_intent\n",
    "\n",
    "class LLMHandler:\n",
    "    classifier = IntentClassifier()  # Instantiate the classifier\n",
    "\n",
    "    @staticmethod\n",
    "    def llm_response(question, summarize_report=False):\n",
    "        \"\"\"\n",
    "        Generate a response for a given question by classifying intent and routing to the appropriate handler.\n",
    "\n",
    "        Args:\n",
    "            question (str): The input question to process.\n",
    "            summarize_report (bool): Whether to generate a report summary (default: False).\n",
    "\n",
    "        Returns:\n",
    "            str: The response generated based on the question intent.\n",
    "        \"\"\"\n",
    "        debug_message(f\"[LLMHandler] Action: Processing question | Question: {question}\")\n",
    "\n",
    "        global context\n",
    "\n",
    "        # Extract patient MRNs from the question\n",
    "        patient_mrns = Util.extract_mrns(question)\n",
    "        if not patient_mrns:\n",
    "            debug_message(\"[LLMHandler] Status: No patient MRNs found in question. Using context MRNs.\")\n",
    "            patient_mrns = context[\"mrns\"]\n",
    "        else:\n",
    "            context[\"mrns\"] = patient_mrns  # Update context with new patient MRNs\n",
    "\n",
    "        # Classify intent using the IntentClassifier\n",
    "        intent = LLMHandler.classifier.classify(question)\n",
    "\n",
    "        # Adjust intent if multiple patient MRNs are in context\n",
    "        if len(context[\"mrns\"]) >= 2 and intent == \"single_qa\":\n",
    "            intent = \"multi_qa\"\n",
    "            debug_message(\"[LLMHandler] Status: Adjusted intent to multi_qa based on context.\")\n",
    "\n",
    "        # Classify based on intent\n",
    "        if intent == \"similar_patient\" and len(patient_mrns) == 1:\n",
    "            debug_message(f\"[LLMHandler] Action: Finding similar patients | Patient MRN: {patient_mrns[0]}\")\n",
    "            return SimilarFinder.find_similar(patient_mrns[0])\n",
    "\n",
    "        elif intent == \"single_qa\" and len(patient_mrns) == 1:\n",
    "            debug_message(f\"[LLMHandler] Action: Answering single patient question | Patient MRN: {patient_mrns[0]}\")\n",
    "            return SingleQA.answer(patient_mrns[0], question)\n",
    "\n",
    "        elif intent == \"multi_qa\" and len(patient_mrns) >= 2:\n",
    "            debug_message(f\"[LLMHandler] Action: Answering multiple patient question | Patient MRNs: {patient_mrns}\")\n",
    "            return MultiQA.multi_answer(question, patient_mrns)\n",
    "\n",
    "        elif intent == \"multi_qa\" and len(patient_mrns) == 0 and len(context[\"mrns\"]) >= 2:\n",
    "            debug_message(f\"[LLMHandler] Action: Answering multiple patient question using context MRNs | Context MRNs: {context['mrns']}\")\n",
    "            return MultiQA.multi_answer(question, context[\"mrns\"])\n",
    "\n",
    "        elif intent == \"summarization\":\n",
    "            if len(patient_mrns) == 1:\n",
    "                debug_message(f\"[LLMHandler] Action: Summarizing report for single patient | Patient MRN: {patient_mrns[0]}\")\n",
    "                return Summarizer.summarize_report(patient_mrns[0])\n",
    "            debug_message(\"[LLMHandler] Action: Summarizing reports for multiple patients.\")\n",
    "            return Summarizer.summarize_patients(patient_mrns)\n",
    "\n",
    "        # Fallback for unsupported or unclear intents\n",
    "        debug_message(\"[LLMHandler] Status: Intent unclear or unsupported. Returning fallback response.\")\n",
    "        return \"I'm sorry, I couldn't understand your request. Could you rephrase it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_context():\n",
    "    global context\n",
    "    context = {\n",
    "        \"ids\": [],\n",
    "        \"docs\": {},\n",
    "        \"specific_docs\": {},\n",
    "        \"chunks\": {},\n",
    "        \"top_chunks\": {}\n",
    "    }\n",
    "\n",
    "reset_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the diagnosis of patient 1?\"\n",
    "response = LLMHandler.llm_response(question)\n",
    "print(\"\\n---------- Response ----------\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
